# Arxiv-Daily

My daily arxiv reading notes.  

[2021 March](202103.md)

[2021 April](202104.md)

[2021 June](202106.md)

## CV (Daily)

#### 20210701

* [SOLO: A Simple Framework for Instance Segmentation](https://arxiv.org/pdf/2106.15947.pdf) 

  > SOLO TPAMI version

* [Align Yourself: Self-supervised Pre-training for Fine-grained Recognition via Saliency Alignment](https://arxiv.org/pdf/2106.15788.pdf)

* [Augmented Shortcuts for Vision Transformers](https://arxiv.org/pdf/2106.15941.pdf)  (Yunhe Wang)

* [Multi-Source Domain Adaptation for Object Detection](https://arxiv.org/pdf/2106.15793.pdf)


#### 20210702
* [CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows](https://arxiv.org/pdf/2107.00652.pdf)  (Dongdong Chen, Nenghai Yu, Baining Guo)
* [AutoFormer: Searching Transformers for Visual Recognition](https://arxiv.org/pdf/2107.00651.pdf) (Jianlong Fu)
* :star: [CLIP-It! Language-Guided Video Summarization](https://arxiv.org/pdf/2107.00650.pdf)  (Trevor Darrell)
* [On the Practicality of Deterministic Epistemic Uncertainty](https://arxiv.org/pdf/2107.00649.pdf)  (Luc Van Gool, Fisher Yu)
* [Global Filter Networks for Image Classification](https://arxiv.org/pdf/2107.00645.pdf)  transformer + 傅里叶变换
* [Focal Self-attention for Local-Global Interactions in Vision Transformers](https://arxiv.org/pdf/2107.00641.pdf)
* :star: [CBNetV2: A Composite Backbone Network Architecture for Object Detection](https://arxiv.org/pdf/2107.00420.pdf)  In this paper, we propose a novel backbone network, namely CBNetV2, by constructing compositions of existing open-sourced pretrained backbones.
* [OPT: Omni-Perception Pre-Trainer for Cross-Modal Understanding and Generation](https://arxiv.org/pdf/2107.00249.pdf)  (Hanqing Lu) we propose an Omni-perception PreTrainer (OPT) for cross-modal understanding and generation, by jointly modeling visual, text and audio resources.
* :star: [Simple Training Strategies and Model Scaling for Object Detection](https://arxiv.org/pdf/2107.00057.pdf)  (Tsung-Yi Lin)
* [CLDA: Contrastive Learning for Semi-Supervised Domain Adaptation](https://arxiv.org/pdf/2107.00085.pdf)
* [Attention Bottlenecks for Multimodal Fusion](https://arxiv.org/pdf/2107.00135.pdf)
* [Learning to See before Learning to Act: Visual Pre-training for Manipulation](https://arxiv.org/pdf/2107.00646.pdf)  (Phillip Isola, Tsung-Yi Lin)
* [Stabilizing Deep Q-Learning with ConvNets and Vision Transformers under Data Augmentation](https://arxiv.org/pdf/2107.00644.pdf)  (Xiaolong Wang)
* :star: [AdaXpert: Adapting Neural Architecture for Growing Data](https://arxiv.org/pdf/2107.00254.pdf)
* [FedMix: Approximation of Mixup under Mean Augmented Federated Learning](https://arxiv.org/pdf/2107.00233.pdf)  (ICLR'21)
* [Scalable Certified Segmentation via Randomized Smoothing](https://arxiv.org/pdf/2107.00228.pdf)  (ICML'21)
* [Revisiting Knowledge Distillation: An Inheritance and Exploration Framework](https://arxiv.org/pdf/2107.00181.pdf)  (CVPR'21, Tongliang Liu, Xinmei Tian, Houqiang Li, Xian-Sheng Hua)
* [Sanity Checks for Lottery Tickets: Does Your Winning Ticket Really Win the Jackpot?](https://arxiv.org/pdf/2107.00166.pdf)