# Arxiv-Daily

My daily arxiv reading notes.  

[2021 March](202103.md)

[2021 April](202104.md)

## CV (Daily)

#### 20210630

* [CLIPDraw: Exploring Text-to-Drawing Synthesis through Language-Image Encoders](https://arxiv.org/pdf/2106.14843.pdf)
* [Rethinking Token-Mixing MLP for MLP-based Vision Backbone](https://arxiv.org/pdf/2106.14882.pdf)
* [A Theory-Driven Self-Labeling Refinement Method for Contrastive Representation Learning](https://arxiv.org/pdf/2106.14749.pdf)
* [Post-Training Quantization for Vision Transformer](https://arxiv.org/pdf/2106.14156.pdf)  (Yunhe Wang, Wen Gao)
* [Semi-supervised Semantic Segmentation with Directional Context-aware Consistency](https://arxiv.org/pdf/2106.14133.pdf)  (Jiaya Jia)
* [An Image Classifier Can Suffice Video Understanding](https://arxiv.org/pdf/2106.14104.pdf)
* [Multimodal Few-Shot Learning with Frozen Language Models](https://arxiv.org/pdf/2106.13884.pdf)
* [Inverting and Understanding Object Detectors](https://arxiv.org/pdf/2106.13933.pdf)
* [K-Net: Towards Unified Image Segmentation](https://arxiv.org/pdf/2106.14855.pdf)
* [Early Convolutions Help Transformers See Better](https://arxiv.org/pdf/2106.14881.pdf)
* [Domain Adaptive YOLO for One-Stage Cross-Domain Detection](https://arxiv.org/pdf/2106.13939.pdf)  (BMVC)


#### 20210615

- [Styleformer: Transformer based Generative Adversarial Networks with Style Vector](https://arxiv.org/pdf/2106.07023.pdf) 
> 基于Transformer的GAN做图像生成 (unconditional)，和SOTA相比comparable
> METHOD: 1. we change the demodulation of StyleGAN2 and modify the existing transformer structure (e.g., residual connection, layer normalization) to create a strong style-based generator with a convolution-free structure;  2.We also make Styleformer lighter by applying Linformer.
> [code](https://github.com/Jeeseung-Park/Styleformer)

- [Exploring and Distilling Posterior and Prior Knowledge for Radiology Report Generation](https://arxiv.org/pdf/2106.06963.pdf) (CVPR'21)
> TASK: Automatically generating radiology reports
> PROBLEM: Yet, this task remains a challenging job for data-driven neural networks, due to the serious visual and textual data biases
> METHOD: To this end, we propose a Posterior-and-Prior Knowledge Exploring-and-Distilling approach (PPKED) to imitate the working patterns of radiologists, who will first examine the abnormal regions and assign the disease topic tags to the abnormal regions, and then rely on the years of prior medical knowledge and prior working experience accumulations to write reports.

- [Cross-Modal Attention Consistency for Video-Audio Unsupervised Learning](https://arxiv.org/pdf/2106.06939.pdf)  (Shaobo Min, Yongdong Zhang, Jingdong Wang)
> MOTIVATION: We human visual perception could attend to regions where sounds are made, and our auditory perception could also ground their frequencies of sounding objects, which we call bidirectional local correspondence. Such supervision is intuitive but not well explored in the contrastive learning framework
> a pretext task, Cross-Modal Attention Consistency (CMAC), aims to align the regional attention generated purely from the visual signal with the target attention generated under the guidance of acoustic signal, and do a similar alignment for frequency grounding on the acoustic attention. 

- [Video Super-Resolution Transformer](https://arxiv.org/pdf/2106.06847.pdf)  (Luc Van Gool)
> TASK: Video super-resolution (VSR)
> PROBLEM: However, the typical block design of Transformer with a fully connected self-attention layer and a tokenwise feed-forward layer does not fit well for VSR due to the following two reasons. First, the fully connected self-attention layer neglects to exploit the data locality because this layer relies on linear layers to compute attention maps. Second, the token-wise feed-forward layer lacks the feature alignment which is important for VSR since this layer independently processes each of the input token embeddings without any interaction among them.
>
> > METHOD: Specifically, to tackle the first issue, we present a spatial-temporal convolutional self-attention layer with a theoretical understanding to exploit the locality information. For the second issue, we design a bidirectional optical flow-based feed-forward layer to discover the correlations across different video frames and also align features.

- [Go Small and Similar: A Simple Output Decay Brings Better Performance](https://arxiv.org/pdf/2106.06726.pdf)
> FUNNY
> This paper begins with empirical observations that better performances are significantly associated with output distributions, that have smaller average values and variances
> By audaciously assuming there is causality involved, we propose a novel regularization term, called Output Decay, that enforces the model to assign smaller and similar output values on each class.

- [Disrupting Model Training with Adversarial Shortcuts](https://arxiv.org/pdf/2106.06654.pdf)
> 一种新的攻击方法：adversarial shortcuts, which encourage models to rely on non-robust signals rather than semantic features.

- [Large-Scale Unsupervised Object Discovery](https://arxiv.org/pdf/2106.06650.pdf)
> Existing approaches to unsupervised object discovery (UOD) do not scale up to large datasets without approximations which compromise their performance. We propose a novel formulation of UOD as a ranking problem. 实验效果惊艳，能够scale到大型数据集上

- [PopSkipJump: Decision-Based Attack for Probabilistic Classifiers](https://arxiv.org/pdf/2106.07445.pdf) (ICML'21)
> Many existing attack algorithms cover various settings, from white-box to black-box classifiers, but typically assume that the answers are **deterministic** and often fail when they are not. We therefore propose a new adversarial decision-based attack **specifically designed for classifiers with probabilistic outputs.**
> [code](https://github.com/cjsg/PopSkipJump)

- [Robust Representation Learning via Perceptual Similarity Metrics](https://arxiv.org/pdf/2106.06620.pdf)  (ICML'21)
> Contrastive Input Morphing (CIM), a representation learning framework that learns input-space transformations of the data to mitigate the effect of irrelevant input features on downstream performance. Our method leverages a perceptual similarity metric via a triplet loss to ensure that the transformation preserves taskrelevant information.

- :star: [Delving Deep into the Generalization of Vision Transformers under Distribution Shifts](https://arxiv.org/pdf/2106.07617.pdf)  (Ziwei Liu)  ViT域适应起步
> In this work, we provide a comprehensive study on the outof-distribution generalization of Vision Transformers
> SETTINGS: we first present a taxonomy of distribution shifts by categorizing them into five conceptual groups: corruption shift, background shift, texture shift, destruction shift, and style shift. Then we perform extensive evaluations of ViT variants under different groups of distribution shifts and compare their generalization ability with Convolutional Neural Network (CNN) models. 
> OBSERVATIONS: 1) ViTs generalize better than CNNs under multiple distribution shifts. With the same or less amount of parameters; 2) Larger ViTs gradually narrow the in-distribution (ID) and outof-distribution (OOD) performance gap.
> To further improve the generalization of ViTs, we design the Generalization-Enhanced Vision Transformers by integrating adversarial learning, information theory, and self-supervised learning. we observe the gradient-sensitivity of Vision Transformers and design a smoother learning strategy to achieve a stable training process.
> Further OBSERVATIONS : 1) For the enhanced model, larger ViTs still benefit more for the out-of-distribution generalization. 2) generalization-enhanced Vision Transformers are more sensitive to the hyper-parameters than their corresponding CNN models.

- [Improved Transformer for High-Resolution GANs](https://arxiv.org/pdf/2106.07631.pdf)
> In this paper, we introduce two key ingredients to Transformer to address this challenge. First, in low-resolution stages of the generative process, standard global self-attention is replaced with the proposed multi-axis blocked self-attention which allows efficient mixing of local and global attention. Second, in high-resolution stages, we drop self-attention while only keeping multi-layer perceptrons reminiscent of the implicit neural function. To further improve the performance, we introduce an additional selfmodulation component based on cross-attention. 降低attention的运算量，做法很直接。

- [Magic Layouts: Structural Prior for Component Detection in User Interface Designs](https://arxiv.org/pdf/2106.07615.pdf)  (CVPR'21) FUNNY APPLICATION

- [PolarStream: Streaming Lidar Object Detection and Segmentation with Polar Pillars](https://arxiv.org/pdf/2106.07545.pdf)
> However, due to use of cartesian coordinate systems these methods represent the sectors as rectangular regions, wasting memory and compute. In this work we propose using a polar coordinate system and make two key improvements on this design.

- [S^2 -MLP: Spatial-Shift MLP Architecture for Vision](https://arxiv.org/pdf/2106.07477.pdf)
> 沿MLP方向的改进，基于spatial-specific造成过拟合的观察，提出spatial-shift module
> The performance drop of MLP-Mixer motivates us to rethink the token-mixing MLP. We discover that token-mixing operation in MLP-Mixer is a variant of depthwise convolution with a global reception field and spatial-specific configuration. But the global reception field and the spatial-specific property make token-mixing MLP prone to over-fitting
> In this paper, we propose a novel pure MLP architecture, spatial-shift MLP (S2 -MLP). Different from MLP-Mixer, our S2 -MLP only contains channel-mixing MLP. We devise a spatial-shift operation for achieving the communication between patches. It has a local reception field and is spatial-agnostic.

- :star: :star: [Partial success in closing the gap between human and machine vision](https://arxiv.org/pdf/2106.07411.pdf)
> Our findings are threefold. (1.) The longstanding robustness gap between humans and CNNs is closing, with the best models now matching or exceeding human performance on most OOD datasets. (2.) There is still a substantial image-level consistency gap, meaning that humans make different errors than models. In contrast, most models systematically agree in their categorisation errors, even substantially different ones like contrastive self-supervised vs. standard supervised models. (3.) In many cases, human-to-model consistency improves when training dataset size is increased by one to three orders of magnitude
> [code](https://github.com/bethgelab/model-vs-human/)

- [Variational Quanvolutional Neural Networks with enhanced image encoding](https://arxiv.org/pdf/2106.07327.pdf)  FUNNY

- [Time Lens: Event-based Video Frame Interpolation](https://arxiv.org/pdf/2106.07286.pdf)  (CVPR'21)

- [Attention-based Domain Adaptation for Single Stage Detectors](https://arxiv.org/pdf/2106.07283.pdf)
> 针对一阶段检测器的域适应目标检测
> previous work has mostly focused on two-stage detectors. This is because their use of region proposals makes it possible to perform local adaptation, which has been shown to significantly improve the adaptation effectiveness.
> To nonetheless benefit from the strength of local adaptation, we introduce an attention mechanism that lets us identify the important regions on which adaptation should focus. Our approach is generic and can be integrated into any single-stage detector.

- [SinIR: Efficient General Image Manipulation with Single Image Reconstruction](https://arxiv.org/pdf/2106.07140.pdf)  (ICML'21, Qifeng Chen)
> We propose SinIR, an efficient reconstructionbased framework **trained on a single natural image** for **general image manipulation, including super-resolution, editing, harmonization, paint-toimage, photo-realistic style transfer, and artistic style transfer**.
> Moreover, with a much simpler training objective (i.e., reconstruction), SinIR is trained 33.5 times faster than SinGAN (for 500 × 500 images) that solves similar tasks.
> [code](https://github.com/YooJiHyeong/SinIR)

- [Survey: Image Mixing and Deleting for Data Augmentation](https://arxiv.org/pdf/2106.07085.pdf)


#### 20210611
- :star: :star: ​[MST: Masked Self-Supervised Transformer for Visual Representation](https://arxiv.org/pdf/2106.05656.pdf)

> 通过attention mask在transformer上实现局部对比学习，对dense的下游任务更有利，性能超越DINO. 
> METHOD: Specifically, inspired by the Masked Language Modeling (MLM) in NLP, we propose a masked token strategy based on the multi-head self-attention map, which dynamically masks some tokens of local patches without damaging the crucial structure for self-supervised learning. More importantly, the masked tokens together with the remaining tokens are further recovered by a global image decoder, which preserves the spatial information of the image and is more friendly to the downstream dense prediction tasks. 


- :star: :star: [Revisiting Contrastive Methods for Unsupervised Learning of Visual Representations](https://arxiv.org/pdf/2106.05967.pdf)  (Luc Van Gool)

> However, current methods are still primarily applied to curated datasets like ImageNet. In this paper, we first study how biases in the dataset affect existing methods. Our results show that an approach like MoCo [22] works surprisingly well across: (i) object- versus scene-centric, (ii) uniform versus long-tailed and (iii) general versus domain-specific datasets.
> Second, given the generality of the approach, we try to realize further gains with minor modifications. We show that learning additional invariances - through the use of multi-scale cropping, stronger augmentations and nearest neighbors - improves the representations.
> Finally, we observe that MoCo learns spatially structured representations when trained with a multi-crop strategy. The representations can be used for semantic segment retrieval and video instance segmentation without finetuning. Moreover, the results are on par with specialized models

- :star: [Learning to See by Looking at Noise ](https://arxiv.org/pdf/2106.05963.pdf) (MIT)

> FUNNY
> Current vision systems are trained on huge datasets, and these datasets come with costs: curation is expensive, they inherit human biases, and there are concerns over privacy and usage rights. To counter these costs, interest has surged in learning from cheaper data sources, such as unlabeled images. In this paper we go a step further and ask if we can do away with real image datasets entirely, instead learning from noise processes. We investigate a suite of image generation models that produce images from simple random processes. These are then used as training data for a visual representation learner with a contrastive loss. 

- :star: [What Does Rotation Prediction Tell Us about Classifier Accuracy under Varying Testing Environments?](https://arxiv.org/pdf/2106.05961.pdf)

> ICML'21  VERY FUNNY
> A natural question then arises: given a trained classifier, can we evaluate its accuracy on varying unlabeled test sets? In this work, we train semantic classification and rotation prediction in a multi-task way. On a series of datasets, we report an interesting finding, i.e., the semantic classification accuracy exhibits a strong linear relationship with the accuracy of the rotation prediction task (Pearson’s Correlation r > 0.88). This finding allows us to utilize linear regression to estimate classifier performance from the accuracy of rotation prediction which can be obtained on the test set through the freely generated rotation labels.

- [Implicit Feature Alignment: Learn to Convert Text Recognizer to Text Spotter](https://arxiv.org/pdf/2106.05920.pdf)

> In this paper, we propose a simple, elegant and effective paradigm called Implicit Feature Alignment (IFA), which can be easily integrated into current text recognizers, resulting in a novel inference mechanism called IFA inference. This enables an ordinary text recognizer to process multi-line text such that text detection can be completely freed. S

- [CAT: Cross Attention in Vision Transformer](https://arxiv.org/pdf/2106.05786.pdf)

> In this paper, we propose a new attention mechanism in Transformer termed Cross Attention, which alternates attention inner the image patch instead of the whole image to capture local information and apply attention between image patches which are divided from single-channel feature maps to capture global information. Both operations have less computation than standard self-attention in Transformer.
> [code](https://github.com/linhezheng19/CAT)

- [Deep neural network loses attention to adversarial images](https://arxiv.org/pdf/2106.05657.pdf)

> 从attention map研究对抗样本的可解释性

- [Space-time Mixing Attention for Video Transformer](https://arxiv.org/pdf/2106.05968.pdf)

> In this work, we propose a Video Transformer model the complexity of which scales linearly with the number of frames in the video sequence and hence induces no overhead compared to an image-based Transformer model.

- [Multi-Dataset Benchmarks for Masked Identification using Contrastive Representation Learning](https://arxiv.org/pdf/2106.05596.pdf)

> 戴口罩情况下的人脸识别

- [Progressive Stage-wise Learning for Unsupervised Feature Representation Enhancement](https://arxiv.org/pdf/2106.05554.pdf)  (Alan Yuille, Bingbing Ni, Wen Gao)

> Progressive Stage-wise Learning (PSL) framework. For a given unsupervised task, we design multilevel tasks and define different learning stages for the deep network. Early learning stages are forced to focus on lowlevel tasks while late stages are guided to extract deeper information through harder tasks.
> 在自监督/无监督学习中引入课程学习/Self-paced Learning

- :star: [Cross-domain Contrastive Learning for Unsupervised Domain Adaptation](https://arxiv.org/pdf/2106.05528.pdf)  (Guo-Jun Qi)

> In this work, we build upon contrastive self-supervised learning to align features so as to reduce the domain discrepancy between training and testing sets. 
> Exploring the same set of categories shared by both domains, we introduce a simple yet effective framework CDCL, for domain alignment. In particular, given an anchor image from one domain, we minimize its distances to cross-domain samples from the same class relative to those from different categories. Since target labels are unavailable, we use a clustering-based approach with carefully initialized centers to produce pseudo labels.
> In addition, we demonstrate that CDCL is a general framework and can be adapted to the data-free setting, where the source data are unavailable during training, with minimal modification

- [Learning to Affiliate: Mutual Centralized Learning for Few-shot Classification](https://arxiv.org/pdf/2106.05517.pdf)  (Deng Cai)

> 现有工作：They generally explore a unidirectional query-to-support paradigm in FSL, e.g., find the nearest/optimal support feature for each query feature and aggregate these local matches for a joint classification. 
> 本文：In this paper, we propose a new method Mutual Centralized Learning (MCL) to fully affiliate the two disjoint sets of dense features in a bidirectional paradigm

- :star: [Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers](https://arxiv.org/pdf/2106.05392.pdf)  (FAIR, Oxford)

> In video transformers, the time dimension is often treated in the same way as the two spatial dimensions. However, in a scene where objects or the camera may move, a physical point imaged at one location in frame t may be entirely unrelated to what is found at that location in frame t + k. These temporal correspondences should be modeled to facilitate learning about dynamic scenes.
> To this end, we propose a new drop-in block for video transformers—trajectory attention—that aggregates information along implicitly determined motion paths. We additionally propose a new method to address the quadratic dependence of computation and memory on the input size, which is particularly important for high resolution or long videos.
> [code](https://github.com/facebookresearch/Motionformer)

- :star: [Beyond BatchNorm: Towards a General Understanding of Normalization in Deep Learning](https://arxiv.org/pdf/2106.05956.pdf)  

> In this work, we take a first step towards this goal by extending known properties of BatchNorm in randomly initialized deep neural networks (DNNs) to nine recently proposed normalization layers.
> Our primary findings follow: (i) Similar to BatchNorm, activations-based normalization layers can avoid exploding activations in ResNets; (ii) Use of GroupNorm ensures rank of activations is at least Ω(pwidth/Group Size), thus explaining why LayerNorm witnesses slow optimization speed; and (iii) Small group sizes result in large gradient norm in earlier layers, hence justifying training instability issues in Instance Normalization and illustrating a speed-stability tradeoff in GroupNorm.


- [AFAN: Augmented Feature Alignment Network for Cross-Domain Object Detection](https://arxiv.org/pdf/2106.05499.pdf)  (Ling Shao)

#### 20210610

* [VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation](https://arxiv.org/pdf/2106.04632.pdf) 效仿GLUE的video-and-language benchmark: an assemblage of 11 video-and-language datasets over 3 popular tasks: (i) text-to-video retrieval; (ii) video question answering; and (iii) video captioning.
* [Check It Again: Progressive Visual Question Answering via Visual Entailment](https://arxiv.org/pdf/2106.04605.pdf) ACL
* :star: [Distilling Image Classifiers in Object Detectors](https://arxiv.org/pdf/2106.05209.pdf)  Nevertheless, the knowledge distillation literature remains limited to the scenario where the student and the teacher tackle the same task. Here, we investigate the problem of transferring knowledge not only across architectures but also across tasks. To this end, we study the case of object detection and, instead of following the standard detector-to-detector distillation approach, introduce a classifier-to-detector knowledge transfer framework.

#### 20210503

* [A Good Image Generator Is What You Need for High-Resolution Video Synthesis](https://arxiv.org/pdf/2104.15069.pdf)  (ICLR'21)
* [Semantic Relation Preserving Knowledge Distillation for Image-to-Image Translation](https://arxiv.org/pdf/2104.15082.pdf)  (ECCV'20)
* [DriveGAN: Towards a Controllable High-Quality Neural Simulation](https://arxiv.org/pdf/2104.15060.pdf)  (CVPR'21, Oral)
* [Faster Meta Update Strategy for Noise-Robust Deep Learning](https://arxiv.org/pdf/2104.15092.pdf)  (Yi Yang)
* [Updatable Siamese Tracker with Two-stage One-shot Learning](https://arxiv.org/pdf/2104.15049.pdf)
* [Unsupervised Data Augmentation for Object Detection](https://arxiv.org/pdf/2104.14965.pdf)
* [Learning Multi-Granular Hypergraphs for Video-Based Person Re-Identification](https://arxiv.org/pdf/2104.14913.pdf)  (CVPR'20, Ling Shao)
* [BiCnet-TKS: Learning Efficient Spatial-Temporal Representation for Video Person Re-Identification](https://arxiv.org/pdf/2104.14783.pdf)
* [CoCon: Cooperative-Contrastive Learning](https://arxiv.org/pdf/2104.14764.pdf)  
* [MOOD: Multi-level Out-of-distribution Detection](https://arxiv.org/pdf/2104.14726.pdf)

##### vision transformers

* [CAT: Cross-Attention Transformer for One-Shot Object Detection](https://arxiv.org/pdf/2104.14984.pdf)
* [End-to-End Attention-based Image Captioning](https://arxiv.org/pdf/2104.14721.pdf)
* [Chop Chop BERT: Visual Question Answering by Chopping VisualBERT’s Heads](https://arxiv.org/pdf/2104.14741.pdf)
* [HandsFormer: Keypoint Transformer for Monocular 3D Pose Estimation of Hands and Object in Interaction](https://arxiv.org/pdf/2104.14639.pdf)
* [Pyramid Medical Transformer for Medical Image Segmentation](https://arxiv.org/ftp/arxiv/papers/2104/2104.14702.pdf)
* [CoSformer: Detecting Co-Salient Object with Transformers](https://arxiv.org/pdf/2104.14729.pdf)
* [Perceptual Image Quality Assessment with Transformers](https://arxiv.org/pdf/2104.14730.pdf)

